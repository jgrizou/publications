%!TEX root = aaai2014.tex
\section{ALGORITHM}
\label{sec:algorithm}

\subsection{Problem definition}

We consider interaction sessions where a machine can perform discrete actions from a set of available actions $a \in \mathcal{A}$ in an either discrete or continuous state space $s \in \mathcal{S}$. The user, that wants to achieve a task $\hat{\xi}$, is providing feedback to the machine using some specific signal $e$, represented as a feature vector. The task is sequential meaning it is completed by performing a sequence of actions. The machine ignores the task the user has in mind, as well as the actual meaning of each user's signal. Its objective is to simultaneously solve the task and learn a model for the user's signals. To achieve this, it has access to a sequence of triplets in the form $D_M = \{(s_i, a_i, e_i),\ i = 1,\ldots,M\}$, where $s_i$, $a_i$ and $e_i$ represent, respectively, the state, action and instruction signals at time step $i$. The behavior of the machine is determined by the actions $a\in\mathcal{A}$ and the corresponding transition model $p(s'\mid s,a)$.


We make the following assumptions under this general paradigm. First, the system has access to a set of tasks $\xi_1,\ldots,\xi_T$ which includes the task the user wants to solve. We assume the instruction signals $e$ have a finite and discrete number of meanings $l \in \{l_1, l_2, \ldots, l_L\}$ which we call labels and this is known by the user and the machine. In this work we will consider two possible meanings for the signals: correct or incorrect; but more complex meanings could be used, such as guidance instructions (go up, go left, ...). We assume that given these labels, it is possible to compute a model that generates or classifies signals $e$ into meanings $l$. The parameters of such a model will be denoted by $\theta$ and we assume this mapping between signal $e$ and their label $l$ to be fixed. However this mapping is unknown to the agent at start.


%The meaning of  model is unknown at start and is parametrize by $\theta$. Given a label $l$ it outputs the probability that an observed signal $e \in \mathbf{R}^n$ was generated by our model $p(e | l, \theta)$.
 
% the signal-to-meaning mapping. This mapping is done by a classifier parameterize by $\theta$, that is unknown at start. Given a signal $e \in \mathbf{R}^n$ the classifier outputs label probabilities $p(l^{H^\theta} | e, \theta)$.

%We further assume that given a task $\xi$ and a state-action pair $(s, a)$, the machine has access to a model of the user expected labels $p(l^{H} | s, a, \xi)$. We denote as $D_i^\xi$ the history of $(e_i, s_i, a_i, p(l_i^{H} | s_i, a_i, \xi))$ up to time $i$ for task $\xi$. 
%Given $D_i^\xi$, we can optimize the classifier parameters $\theta_i^{\xi}$.


% For each hypothesis, at time $i$, we have several dataset $D_i^{\xi_t}$, which are used to train a classifier parametrize by $\theta_i^{\xi_t}$. Each classifier $\theta_i^{\xi_t}$ represents an interpretation hypothesis of the user given he is trying to instruct the task $\xi_t$.

\subsection{Estimating Tasks Likelihoods}

% \todo{Conceptual terms, no Gaussian classifier, no EEG}

We start by assuming we are provided a signal decoder $\hat{\theta}$ and relax this assumption later on.
%
As mentioned in the introduction, knowing $\hat{\theta}$, we can compute the probability of each task $\xi_t$ after observation of a signal $e$ when performing action $a$ in state $s$:
%
\begin{eqnarray}
p(\xi_t|e, s, a, \hat{\theta}) & \propto & p(e |s, a, \hat{\theta}, \xi_t) p(\xi_t)
\label{eq:1}
\end{eqnarray}
%
where  $p(e |s, a, \hat{\theta}, \xi_t)$ needs to take into account the probability of each possible meaning $l$ given the target $\xi_t$, the current state $s$ and the action $a$ executed by the machine:
%
\begin{eqnarray}
p(e |s, a, \hat{\theta}, \xi_t) =  \sum_{k \in {1, \ldots, L}} p(e |l = l_k, \hat{\theta}) p(l = l_k| s, a, \xi_t)
\end{eqnarray}

This process can be repeated recursively for several interaction steps $i$:
%
\begin{eqnarray}
\L_i^{\xi_t} & = & p(\xi_t|D_i^{\xi_t}, \hat{\theta}) \nonumber \\
& \propto & p(e_i |s_i, a_i, \hat{\theta},\xi_t) p(\xi_t|D_{i-1}^{\xi_t}, \hat{\theta})
\label{eq:filter}
\end{eqnarray}
%
with $p(\xi_t|D_0^{\xi_t}, \hat{\theta})$ being the prior at time 0 (before the experiment starts) for the task $\xi_t$, usually uniform over the task distribution. 

% Also, if we knew what task the user had in mind at every time step ($\xi_{1:i}$), one could estimate $p(\theta \mid D_i, \xi_{1:i})$, that is, the posterior distribution of the meaning parameters given the task being solved at each point in time. Note the different notation for the task and the meaning parameters. The reason is that $\theta$ is a shared parameter for different tasks, while the task being solved should in principle not affect the meaning of the signals $e$.

%By normalizing the likelihoods of all task, we obtain the probability distribution between tasks. A threshold mechanism could be used to decide when to stop.

We now relax the assumption we are given a model $\hat{\theta}$. The natural extension from the previous models is to compute the posterior distribution over the task and the model, $p(\xi, \theta |e, s, a)$. However, the resulting distribution does not have a close form solution even when linear Gaussian likelihoods are used due to the combination of mixtures for each possible task. Another alternative is to compute the $\theta$ and $\xi$ that maximize the data likelihood. This is prone to fail in certain scenarios due to two reasons. First, it is common that different tasks share many labels (e.g. the policies to reach neighboring cells on a grid world are almost identical and, therefore, share most of the labels $l$) and results on large uncertainties in the task space that require multiple actions to be disambiguated. Second, if the signals are not well separated the meaning parameters $\theta$ of different tasks will not differ much.  

For instance, under Gaussian assumptions for $p(e |l = l_k, \theta)$ and deterministic task labels $p(l = l_k| s, a, \xi)$, it is possible to integrate out  $\theta$ to compute the marginal likelihood $p(D_M\mid \xi)$. The resulting likelihood depends only on the traces of each $p(e |l = l_k, \theta)$. Empirical results with synthetic and EEG data for a reaching task on a grid revealed that, when the distributions over $e$ overlap, the traces were not enough to recover the most likely task and the corresponding meaning parameters. 

To cope with these problems, we define the following pseudo-likelihood function:
%
\begin{eqnarray}\label{eq:pseudo1}
\lefteqn{P(D_M | \xi, \theta) \approx \prod_{i=1}^M p(e_i | s_i, a_i, \xi, \theta_{-i})}  \\ \label{eq:pseudo2}
&=& \prod_{i=1}^M \sum_{l_c}\sum_l  p(e_i | l_c, \theta_{-i})  p(l_c | l, \theta_{-i}) p(l|s_i,a_i,\xi)
%p(\xi_t, \theta^{\xi_t} |e, s, a) & \propto & p(e | s, a, \xi_t, \theta^{\xi_t}) p(\xi_t, \theta^{\xi_t}).
\label{eq:}
\end{eqnarray}
%
where $l$ represents the meaning assigned by task $\xi$, action $a_i$ and state $s_i$ and $l_c$ is the label corrected based on what we know about our classifier $\theta_{-i}$ for a given label $l$.

The pseudo-likelihood is built using a leave-one-out cross-validation strategy to evaluate the likelihood $p(e_i | s_i, a_i, \xi, \theta_{-i})$ of each signal based on the meaning parameters $\theta_{-i}$ learned for each task using all the other available signals. The use of $\theta_{-i}$ indicates we use a leave one out method. If we interpret $p(e_i | s_i,a_i,\xi,\theta_{-i})$ as a classifier, its predicted labels should match the ones provided by the task for different state-actions pairs. The rationale behind it is that for the correct task, the signals and labels will be more coherent than for other tasks, which we measure as the predictive ability of a classifier trained on the signal-label pairs. Note that wrong tasks will assign wrong labels $l$ to the signals $e$, therefore the learned models will have larger overlaps (see Figure~\ref{fig:planningExplained}c). 

% resulting in higher likelihoods. Coherence between signals and labels is measured as the predictive ability of a classifier trained on the signal-label pairs. The use of $\theta_{-i}$ indicates we use a leave one out method. If we interpret $p(e_i | \xi, \theta_{-i},s_i,a_i)$ as a classifier, its predicted labels should match the ones provided by the task for different state-actions pairs. Note that wrong tasks will assign wrong labels $l$ to the signals $e$ and, therefore, the learned models will have larger overlaps. See the discussio of Figure~\ref{fig:planningExplained}. 

Each term of the pseudo-likelihood is computed from three terms. $p(l|s_i,a_i,\xi)$ represents the probability distributions of the meanings according to a task, the executed action and the current state.   $p(l_c | l, \theta_{-i})$ encodes which label will be actually recovered by $\theta_{-i}$. Intuitively, it models the quality of the model $\theta_{-i}$. $p(e_i | l_c,\theta_{-i})$ is the likelihood of the signal given the meaning. 
%
The pseudo-likelihood is maximized in two steps. First, the maximum a posteriori estimate $\theta_{-i}$ of each task is computed. Then, the term $p(l_c | l, \theta_{-i})$ is approximated by the corresponding confusion matrix of the classifier based on $\theta_{-i}$. It is the probability that the classifier itself is reliable in its prediction. Finally, the best task $\xi$ should be the one that maximizes the pseudo-likelihood in Eq. \ref{eq:pseudo1}.

%
%A sensible option is to perform independant update of classifier and task. For each task, at time $i$, we have a dataset $D_i^{\xi_t}$, which can be used to train a classifier parametrize by $\theta_i^{\xi_t}$. Each classifier represents an interpretation hypothesis of the user signals given he is instructing the task $\xi_t$.
%
%Incorporating $\theta_i^{\xi_t}$ in equation \ref{eq:filter} we obtain:
%
%\begin{eqnarray}
%\L_i^{\xi_t} & = & p(\xi_t|D_i^{\xi_t}, \theta_i^{\xi_t}) \nonumber \\
%& \propto & p(e_i |s_i, a_i, \theta_i^{\xi_t}, \xi_t) p(\xi_t|D_{i-1}^{\xi_t}, \theta_{i-1}^{\xi_t})
%\label{eq:filter}
%\end{eqnarray}
%with $p(\xi_t|D_0^{\xi_t}) = \frac{1}{T}$.
%
%\todo{no $p(\theta)$, it is not good, maybe
%\begin{eqnarray}
%\L_i^{\xi_t} & = & \int_{\theta_i^{\xi_t}} p(\xi_t|D_i^{\xi_t}, \theta_i^{\xi_t}) p(\theta_i^{\xi_t}) d\theta_i^{\xi_t} \nonumber \\
%\label{eq:filter}
%\end{eqnarray}
%}
%
%Unfortunatly as the models used to evaluate the new evidences are different for each task hypothesis we can not compare directly the likelihoods by simply normalizing them.
%
%
%In order to weight appropriatly the estimates, we would like to known if the prediction of our model can be trusted \todo{this shouldn't be $p(\theta)$ in the model above?}. As we do not have access to the true repartition of signal or the true model we must rely on other empricial metric. Our hypothesisis is that for the correct task the system will associated the correct label. The resulting dataset should be more separable than the others who will mix the labels (see figure \ref{fig:planningExplained}c). To estimate this separability, we compute the classification performance of our model on the labeled data. Our system is looking for the hypothesis from which emerges a coherence between the spacial organization of label in the feature space and their associated labels.
%
%Hence we define a new metric in the label space. We will compare the expected user labels $p(l^{\xi_t} | s, a, \xi_t)$ with the labels intended by the user $p(l^{\theta} | e, \theta)$. The more the predictions match, the more likely the task. By using this estimate we can include a performance measure on the classifier prediction $p(l^{\theta} | e, \theta)$ by relying a cross validation procedure. \todo{maybe we can explain other approcahes, like estimating the overlap of the two distributions}
%
%We can compute the probability:
%\begin{eqnarray}
%J^{\xi_t}(e,s,a) & = & \sum_{k \in {1, \ldots, L}}  p(l^{\xi_t} = k | s, a, \xi_t) p(l^{\theta^{\xi_t}} = k | e, \theta^{\xi_t}) \nonumber \\
%\end{eqnarray}
%With $p(l^{\theta^{\xi_t}} = k | e, \theta^{\xi_t})$ computed via a cross validation procedure. \todo{it mean that there is several $\theta$ used here, maybe replacing them with $D_i^{\xi_t}$}
%
%\todo{At each new step, we can compute the likelihood of each task by cumulating the evidence.}
%
%\begin{eqnarray}
%\L^{\xi_t} = \prod_{i \in {1, \ldots, M}} J^{\xi_t}(e_i,s_i,a_i)
%\end{eqnarray}

\subsection{Decision and Task Change}

% The machine is simultaneously solving a task and learning the meanings associated to the signals. However, it is possible to solve the task before the meanings are perfectly modeled or vice-versa. Therefore, in practical terms it is necessary to be able to switch to the next task once it has been solved and to continue updating the meaning. 

The machine must decide which task is the correct one. To do so, we define $W^{\xi_t}$ the minimum of pairwise normalized likelihood between hypothesis $\xi_t$ and each other hypothesis: 
%
\begin{eqnarray}
W^{\xi_t} = \min_{x~\in~{1, \ldots, T} \smallsetminus \{t\}} \frac{P(D_M | \xi_t, \theta)}{P(D_M | \xi_t, \theta) + P(D_M | \xi_x, \theta)}
\label{eq:weight}
\end{eqnarray}

When it exists a $t$ such that $W^{\xi_t}$ exceeds a threshold $\beta \in ]0.5,1]$ we consider task $\xi_t$ is the one taught by the user.

Once a task is identified with confidence, the robot executes it and prepares to receive instructions from the user to execute a new task. Assuming the user starts teaching a new task using the same kind of signals, we now have much more information about the signal model. Indeed, we are confident that the user was providing instructions related to the previously identified task; therefore we can infer the true labels of the past signals. We can now  assign such labels to all hypothesis and by using the same algorithm we can start learning the new task faster as all hypothesis now share a common set of signal-label pairs. The meaning models for each hypothesis are still updated step after step until the new task is identified and labels reassigned.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}

At next time step $i+1$ we can compute the probability:
\begin{flalign}
& J_{i+1}^{\xi_t}(s_{i+1},a_{i+1},e_{i+1}) = \nonumber \\
& = \sum_{l \in {1, \ldots, L}} p(l_{i+1}^{H^{\xi_t}} = l \cap l_{i+1}^{H^{\theta_{i}^{\xi_t}}} = l | e_{i+1}, s_{i+1}, a_{i+1}, \xi_t, \theta_{i}^{\xi_t}) \nonumber \\
& = \sum_{l \in {1, \ldots, L}}  p(l_{i+1}^{H^{\xi_t}} =l | s_{i+1}, a_{i+1}, \xi_t) p(l_{i+1}^{H^{\theta_i^{\xi_t}}} = l | e_{i+1}, \theta_i^{\xi_t})
\end{flalign}
given a classifier $\theta_i^{\xi_t}$ trained on past interaction $D_i^{\xi_t}$. As the classifier changes every step, equation \ref{eq:joint} becomes recursive:

\begin{eqnarray}
\L_{i+1}^{\xi_t} =   %\L_{i}^{\xi_t} \sum_{l \in {1, \ldots, L}} p(l_{i+1}^{\xi_t} = l\cap l_{i+1}^{\theta_i^{\xi_t}} = l | e_{i+1}, s_{i+1}, a_{i+1}, \xi_t, \theta_i^{\xi_t}) \nonumber \\
%& = &
J_{i+1}^{\xi_t}(s,a,e)~~\L_{i}^{\xi_t} 
 % \sum_{l \in {1, \ldots, L}} p(l_{i+1}^{H^{\xi_t}} =l | s_{i+1}, a_{i+1}, \xi_t) \ldots \nonumber \\
% \ldots p(l_{i+1}^{H^{\theta_i^{\xi_t}}} = l | e_{i+1}, \theta_i^{\xi_t})
\label{eq:algo}
\end{eqnarray}
with $\forall t, \L_{0}^{\xi_t} = 1$.

\textbf{Why would it works?} Because the user is behaving only according to one task hypothesis and the coherence between spacial organization of signals and their associated labels should be higher for the correct hypothesis, see figure \ref{fig:intuition}. The chosen classifier --- by its assumptions, constraints and biases --- indirectly measure this coherence by making inconsistent predictions. Accounting for such uncertainty in predictions is primordial for our algorithm to work.

\subsection{Modeling Classifier Uncertainty}

At every time step, we empirically model the uncertainty of our classifiers predictions by estimating their associated confusion matrix. We use a 10 fold cross validation procedure on the training data $D_i^{\xi_t}$. The confusion matrix gives us the probability that the true label was $i$ given our classifier prediction was $j$. It is used to temperate the predictions $p(l_{i+1}^{H^{\theta_i^{\xi_t}}} = l | e_{i+1}, \theta_i^{\xi_t})$ which decomposes as:

\begin{eqnarray}
\sum_{k = 1,\ldots, L} p(l_{i+1}^{H^{\theta_i^{\xi_t}}} = l| l_{i+1}^{\theta_i^{\xi_t}} = k) p(l_{i+1}^{\theta_i^{\xi_t}} = k | e_{i+1}, \theta_i^{\xi_t})
\label{eq:conf}
\end{eqnarray}
with $p(l_{i+1}^{\theta_i^{\xi_t}} = k | e_{i+1}, \theta_i^{\xi_t})$ the raw output of the classifier and $p(l_{i+1}^{H^{\theta_i^{\xi_t}}} = l| l_{i+1}^{\theta_i^{\xi_t}} = k)$ given by the confusion matrix 

\subsection{Decision}

We define $W^{\xi_t}$ the minimum of pairwise normalized likelihood between hypothesis $\xi_t$ and each other hypothesis:

\begin{eqnarray}
W^{\xi_t} = \argmin_{x~\in~{1, \ldots, T} \smallsetminus \{t\}} \frac{\L^{\xi_t}}{\L^{\xi_t} + \L^{\xi_x}}
\label{eq:weight}
\end{eqnarray}

When there exists a $t$ such that $W(\xi_t)$ exceed a threshold $\beta \in ]0.5,1]$ we consider task $\xi_t$ is the one taught by the user.

\subsection{From Task 1 to Task 2}

After identifying a first task, we can infer the true labels of the past data and assign such labels for all hypothesis. At this iteration only, all hypothesis have a similar classifier. The user starts teaching a new task using the same kind of signals. By using the same algorithm we can start learning this new task faster as all hypothesis share a common set of signal-label pairs. The classifier keeps being updated step after step until next task is identified and labels reassigned.

\end{comment}


